# Transformer Resources

## articles

* [Adam: A Method for Stochasitc Optimization, D. Kingma et al, 2014](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Adam-A_method_for_stochastic_optimization_Kingma_2014.pdf)

* [Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning, Roemmele et al, 2011](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Choice_of_Plausible_Alternatives-An_Evaluation_of_Commonsense_Causal_Reasoning_Roemmele_2011.pdf)

* [Catastrophic Interference In Connectionist Networks: The Sequential Learning Problem, McCloskey, Cohen, 1989](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Catastrophic_Interference_in_Connectionist_Networks-The_Sequential_Learning_Problem_Mccloskey_Cohen_1989.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., Google AI, 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling, Ott et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FairseqAFastExtensibleToolkitForSequenceModeling.pdf)

* [Autoencoders, Dor Bank et al, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Autoencoders.pdf)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung et al., 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/EmpiricalEvaluationOfGatedRecurrentNeuralNetworksonSequenceModeling.pdf)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Cho et al., U de Montreal, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, A. Sherstinsky, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FundamentalsOfRNNandLSTMNetwork.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)

* [Sequence to Sequence Learning with Neural Networks, Sutskever et al, Google Research, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SequencetoSequenceLearningwithNeuralNetworksSutsekver2014.pdf)

* [Transforming Auto-encoders, G. Hinton, A. Krizhevsky, et al., 2011](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TransformingAutoencodersHinton.pdf)

* [Long Short-Term Memory, Sepp Hochreiter et al., 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/LongShortTermMemory.pdf)

* [Understanding LSTM: a tutorial into Long Short-Term Memory, R. Staudemeyer et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TutorialOnLongShortTermMemory2019.pdf)

* [Meta-Transformer: A Unified Framework for Multimodal Learning, Zhang, Y, et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Meta-Transformer_Unified_Framework_for_Multi-Modal_Learning_2023.pdf)

* [Small-scale proxies for large-scale Transformer training instabilities, M. Wortsman et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Small-scale_proxies_for_large-scale_Transformer_training_instabilities_Wortsman_2023.pdf)

* [Formal Algorithms for Transformers, M. Phuong et al, DeepMind, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Formal_Algorithms_for_Transformers_Phuong_DeepMind_2022.pdf)

* [Boolformer: Symbolic Regression of Logic Functions with Transformers, d'Ascoli et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Transformer-Based Direct Hidden Markov Model for Machine Translation, W. Wang et al, Aachen U, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/markov/Transformer-Based_Direct_Hidden_Markov_Model_for_Machine_Translation_Wang_AachenU_2021.pdf)

## In-context learning with Transformers

* [Hierarchical Attention Networks for Document Classification, Z. Yang et al, CMU, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Hierarchical_Attention_Networks_for_Document_Classification_Yang_CMU_2016.pdf)

* [Attention using Context Vector: Hierarchical Attention Networks for Document Classification, DataScience.StackExchange, 2017](https://datascience.stackexchange.com/questions/19346/attention-using-context-vector-hierarchical-attention-networks-for-document-cla)

* [What is the difference between positional vector and attention vector used in transformer model? DataScience.StackExchange, 2019](https://datascience.stackexchange.com/questions/77093/what-is-the-difference-between-positional-vector-and-attention-vector-used-in-tr)

* [Why does an attention layer in a transformer learn context?, DataScience.StackExchange, 2020](https://datascience.stackexchange.com/questions/85301/why-does-an-attention-layer-in-a-transformer-learn-context)

* [Data Distributional Properties Drive Emergent In-Context Learning in Transformers, S. Chan et al, DeepMind, NeurIPS, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/NeurIPS-2022-data-distributional-properties-drive-emergent-in-context-learning-in-transformers-Paper-Conference.pdf)

* [In-Context Learning with Transformer-Based Neural Sequence Models, Jair Ribeiro, Towards AI publication, 2023](https://pub.towardsai.net/in-context-learning-with-transformer-based-neural-sequence-models-136af9848d15)

* [In-context Learning and Induction Heads, Catherine Olsson et al, Anthropic, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/In-context_Learning_and_Induction_Heads_Olsson_Anthropic_2023.pdf)

* [Transformers Learn In-Context by Gradient Descent, Johannes von Oswald et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Transformers_Learn_In-Context_by_Gradient_Descent_von_Oswald_2023.pdf)

* [Transformers as Algorithms: Generalization and Stability in In-context Learning, Y. Li et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Transformers_as_Algorithms-Generalization_and_Stability_in_In-context_Learning_Li_2023.pdf)

* [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, S. Garg, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/What_Can_Transformers_Learn_In-Context_A_Case_Study_of_Simple_Function_Classes_Garg_2023.pdf)

    related repo: https://github.com/dtsip/in-context-learning

    related youtube presentation: https://www.youtube.com/watch?v=DiJsg93zQDc

* [The Transient Nature of Emergent In-Context Learning in Transformers, A. Singh et al, UCL, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/The_Transient_Nature_of_Emergent_In-Context_Learning_in_Transformers_Singh_UCL_2023.pdf)

* [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions, S. Bhattamishra, Oxford U., 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Understanding_In-Context_Learning_in_Transformers_and_LLMs_by_Learning_to_Learn_Discrete_Functions_Bhattamishra_2023.pdf) 

* [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models, D. Fu et al, USC, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Transformers_Learn_Higher-Order_Optimization_Methods_for_In-Context_Learning-A_Study_with_Linear_Models_Fu_USC_2023.pdf)

* [Learning linear models in-context with transformers with Spencer Frei (UC Davis), Imperial College London, youtube video](https://www.youtube.com/watch?v=_lf7CD8zRyQ)

* [In-context Learning in Transformers - SLT Seminar 46, youtube video](https://www.youtube.com/watch?v=Y00HtwKO3BY)

## Reinforcement Learning in Transformers

* [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions, Yevgen Chebotar et al, DeepMind, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Q-Transformer-Scalable_Offline_Reinforcement_Learning_via_Autoregressive_Q-Functions_Chebotar_DeepMind_2023.pdf)

    related repo: https://qtransformer.github.io/


[... More articles on Transformers](https://github.com/dimitarpg13/transformers_intro/tree/main/articles_and_books)

## Medium
* [The A-Z of Transformers: Everything You Need to Know with François Porcher](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
    
    related paper: [Neural Machine Translation by Jointly Learning to Align and Translate, D. Bahdanau et al, 2015](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/NeuralMachineTranslationByJointlyLearningToAlignAndTranslateBahdanau2015.pdf)

    related repo: [Transformers from Scratch](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch)

* [Transformers — Intuitively and Exhaustively Explained with Daniel Warfield](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)

* [De-coded: Transformers explained in plain English with Chris Hughes](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429)

* [Explainable AI: Visualizing Attention in Transformers with Abby Morgan](https://generativeai.pub/explainable-ai-visualizing-attention-in-transformers-4eb931a2c0f8)

* [Transformers - The Bigger The Better? with Jordi Torres](https://towardsdatascience.com/transformers-the-bigger-the-better-19f39f222ee3)

* [How to Take Advantage of the New Disruptive AI Technology Called Transformers with Jordi Torres](https://torres-ai.medium.com/how-to-take-advantage-of-the-new-disruptive-ai-technology-called-transformers-9e57a26506cb)

* [Transformes: The New Gem of Deep Learning with Jordi Torres](https://torres-ai.medium.com/transformers-the-new-gem-of-deep-learning-d0ae04bc4a75)

* [Transfer Learning: The Democratization of Transformers with Jori Torres](https://torres-ai.medium.com/transfer-learning-the-democratization-of-transformers-1d2493b14883)

* [Visualizing Attention in Vision Transformer with Aryan Jadon](https://medium.com/@aryanjadon/visualizing-attention-in-vision-transformer-c871908d86de)

* [The Transformer Architecture of GPT Models with Beatriz Stollniz](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)

* [Learning Transformers Code First Part 1 - The Setup with Lily Hughs-Robinson](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

* [Learning Transformers Code First Part 2 - GPT Up Close and Personal with Lily Hughs-Robinson](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)

* [Were Abstract Painters The First Encoders with Wouter van Heeswijk](https://towardsdatascience.com/were-abstract-painters-the-first-encoders-49aa1e04ffd5)

* [Comparison of Convolutional Neural Networks and Vision Transformers (ViTs) with Illas Papastratis](https://medium.com/@iliaspapastratis/comparison-of-convolutional-neural-networks-and-vision-transformers-vits-a8fc5486c5be)

    related paper: [Do Vision Transformers See Like Convolutional Neural Networks? M. Raghu, Google Brain, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Do_Vision_Transformers_See_Like_Convolutional_Neural_Networks_Raghu_GoogleBrain_2021.pdf)

    related paper: [How Do Vision Transformers Work? N. Park et al, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/How_Do_Vision_Transformers_Work_Park_2022.pdf)

    related paper: [An Image is Worth 16X16 Wwords: Transformers for Image Recognition at Scale, A. Dosovitskiy, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

    related blog: https://blog.research.google/2020/12/transformers-for-image-recognition-at.html

    related blog: https://becominghuman.ai/transformers-in-vision-e2e87b739feb

    related blog: https://viso.ai/deep-learning/vision-transformer-vit/

* [Understanding Temporal Fusion Transformer with Mouna Labiadh](https://medium.com/@mouna.labiadh/understanding-temporal-fusion-transformer-9a7a4fcde74b)

    related article: [Temporal Fusion Transformer for Interpretable Multi-horizon Time Series Forecasting](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TemporalFusionTransformersforInterpretableMulti-horizonTimeSeriesForecasting_BrianLim_2021.pdf)

* [Forecasting book sales with Temporal Fusion Transformer with Mouna Labiadh](https://medium.com/@mouna.labiadh/forecasting-book-sales-with-temporal-fusion-transformer-dd482a7a257c)

* [Personalized Recommendations with Transformers with Enis Teper](https://medium.com/hepsiburada-data-science/personalized-recommendations-with-transformers-11c13cff2be)

* [Hidden Markov Models Simplified with Sanjay Dorairaj](https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab)

* [Rubik’s cubes and Markov chains with Eduardo Teste](https://towardsdatascience.com/a-random-walk-on-the-rubiks-cube-684dd304bf3e)
