# Transformer Resources

## books

* [Natural Language Processing with Transformers, Lewis Tunstall, Leandro von Werra, Thomas Wolf, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/natural-language-processing-with-transformers-revised-edition-book.pdf)

## articles

* [Adam: A Method for Stochasitc Optimization, D. Kingma et al, 2014](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Adam-A_method_for_stochastic_optimization_Kingma_2014.pdf)

* [Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning, Roemmele et al, 2011](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Choice_of_Plausible_Alternatives-An_Evaluation_of_Commonsense_Causal_Reasoning_Roemmele_2011.pdf)

* [Catastrophic Interference In Connectionist Networks: The Sequential Learning Problem, McCloskey, Cohen, 1989](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Catastrophic_Interference_in_Connectionist_Networks-The_Sequential_Learning_Problem_Mccloskey_Cohen_1989.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [Deriving Machine Attention from Human Rationales, Y. Bao et al, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Deriving_Machine_Attention_from_Human_Rationales_Bao_IBM-Watson_2018.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., Google AI, 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling, Ott et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FairseqAFastExtensibleToolkitForSequenceModeling.pdf)

* [Autoencoders, Dor Bank et al, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Autoencoders.pdf)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung et al., 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/EmpiricalEvaluationOfGatedRecurrentNeuralNetworksonSequenceModeling.pdf)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Cho et al., U de Montreal, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, A. Sherstinsky, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FundamentalsOfRNNandLSTMNetwork.pdf)

* [Bidirectional Recurrent Neural Networks, Mike Schuster, Kuldip Paliwal, 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Bidirectional_Recurrent_Neural_Networks_Schuster_Paliwal_1997.pdf)

* [Neural Networks for Pattern Recognition, C. M. Bishop, 1995](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Neural_Networks_for_Pattern_Recognition-Bishop_1995.pdf)

* [Translation Modeling with Bidirectional Recurrent Neural Networks, M. Sundermeyer, et al, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Translation_Modeling_with_Bidirectional_Recurrent_Neural_Networks_Sundermeyer_2014.pdf)

* [Simple and Accurate Dependency Parsing Using Bidirectional LSTM Feature Representations, E. Kiperwasser, Y. Goldberg, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Simple_and_Accurate_Dependency_Parsing_Using_Bidirectional_LSTM_Feature_Representations_Kiperwasser_2016.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)

* [Sequence to Sequence Learning with Neural Networks, Sutskever et al, Google Research, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SequencetoSequenceLearningwithNeuralNetworksSutsekver2014.pdf)

* [Transforming Auto-encoders, G. Hinton, A. Krizhevsky, et al., 2011](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TransformingAutoencodersHinton.pdf)

* [A Neural Probabilistic Language Model, Y. Bengio et al, 2003](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/A_Neural_Probabilistic_Language_Model_bengio03a.pdf)

* [Learning to combine foveal glimpses with a third-order Boltzmann machine, H. Larochelle and G. Hinton, 2010](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/NIPS-2010-learning-to-combine-foveal-glimpses-with-a-third-order-boltzmann-machine-Paper.pdf)

* [Long Short-Term Memory, Sepp Hochreiter et al., 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/LongShortTermMemory.pdf)

* [LSTM Can Solve Hard Long Time Lag Problems, Sepp Hochreiter, Juergen Schmidthhuber, NIPS, 1996](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/NIPS-1996-lstm-can-solve-hard-long-time-lag-problems-Paper_Hochreiter.pdf)

* [End-to-End Continuous Speech Recognition using Attention-based Recurrent NN: First Results, Jan Chorowski, Dzmitry Bandanau et al, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/End-to-end_Continuous_Speech_Recognition_using_Attention-based_Recurrent_NN-First_Results_Chorowski_2014.pdf)

* [Recurrent Continuous Translation Models, Nal Kalchbrenner, Phil Blunsom, Oxford U, 2013](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Recurrent_continuous_translation_models_Kalchbrenner_Blunsson_OxfordU_2013.pdf)

* [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, K. Cho, Dzmitry Bahdanau, et al, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Learning_Phrase_Representations_using_RNN_Encoder%E2%80%93Decoder_for_Statistical_Machine_Translation_Cho_UdMontreal_2014.pdf)

* [Understanding LSTM: a tutorial into Long Short-Term Memory, R. Staudemeyer et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TutorialOnLongShortTermMemory2019.pdf)

* [Generating Sequences with Recurrent Neural Networks, Alex Graves, UofToronto, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Generating_Sequences_With_Recurrent_Neural_Networks_Graves_2014.pdf)

* [Meta-Transformer: A Unified Framework for Multimodal Learning, Zhang, Y, et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Meta-Transformer_Unified_Framework_for_Multi-Modal_Learning_2023.pdf)

* [Small-scale proxies for large-scale Transformer training instabilities, M. Wortsman et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Small-scale_proxies_for_large-scale_Transformer_training_instabilities_Wortsman_2023.pdf)

* [Formal Algorithms for Transformers, M. Phuong et al, DeepMind, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Formal_Algorithms_for_Transformers_Phuong_DeepMind_2022.pdf)

* [Boolformer: Symbolic Regression of Logic Functions with Transformers, d'Ascoli et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Transformer-Based Direct Hidden Markov Model for Machine Translation, W. Wang et al, Aachen U, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/markov/Transformer-Based_Direct_Hidden_Markov_Model_for_Machine_Translation_Wang_AachenU_2021.pdf)

* [Simplifying Transformer Blocks, Bobbe He et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SimplifyingTransformerBlocks_He2023.pdf)

* [Introduction to Transformers: an NLP Perspective, T. Xiao et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Introduction_to_Transformers-a_NLP_Perspective_Xiao_2023.pdf)

## Embeddings

* [Embeddings in Natural Language Processing: Theory and Advances in Vector Representation of Meaning, M. Pilhevar, J. Camacho-Collados, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/embeddings/Embeddings_in_NLP_Pilhevar_book_draft.pdf)

* [Efficient Estimation of Word Representations in Vector Space, Tomas Mikolov et al, Google, 2013](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/embeddings/Efficient_Estimation_of_Word_Representations_in_Vector_Space_Mikolov_Google_2013.pdf)

* [Factors Influencing the Surprising Instability of Word Embeddings, L. Wendtlandt et al, U. Michigan Ann Arbor, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/embeddings/Factors_Influencing_the_Surprising_Instability_of_Word_Embeddings_Wendlandt_AnnArbor_2018.pdf)

* [Is Cosine-Similarity of Embeddings Really About Similarity? Harald Steck, Chaitanya Ekanadham, 2024](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/embeddings/Is_Cosine-Similarity_of_Embeddings_Really_About_Similarity_Steck_2024.pdf)

## In-context learning with Transformers

* [Hierarchical Attention Networks for Document Classification, Z. Yang et al, CMU, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Hierarchical_Attention_Networks_for_Document_Classification_Yang_CMU_2016.pdf)

* [Attention using Context Vector: Hierarchical Attention Networks for Document Classification, DataScience.StackExchange, 2017](https://datascience.stackexchange.com/questions/19346/attention-using-context-vector-hierarchical-attention-networks-for-document-cla)

* [What is the difference between positional vector and attention vector used in transformer model? DataScience.StackExchange, 2019](https://datascience.stackexchange.com/questions/77093/what-is-the-difference-between-positional-vector-and-attention-vector-used-in-tr)

* [Why does an attention layer in a transformer learn context?, DataScience.StackExchange, 2020](https://datascience.stackexchange.com/questions/85301/why-does-an-attention-layer-in-a-transformer-learn-context)

* [Data Distributional Properties Drive Emergent In-Context Learning in Transformers, S. Chan et al, DeepMind, NeurIPS, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/NeurIPS-2022-data-distributional-properties-drive-emergent-in-context-learning-in-transformers-Paper-Conference.pdf)

* [In-Context Learning with Transformer-Based Neural Sequence Models, Jair Ribeiro, Towards AI publication, 2023](https://pub.towardsai.net/in-context-learning-with-transformer-based-neural-sequence-models-136af9848d15)

* [In-context Learning and Induction Heads, Catherine Olsson et al, Anthropic, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/In-context_Learning_and_Induction_Heads_Olsson_Anthropic_2023.pdf)

* [Transformers Learn In-Context by Gradient Descent, Johannes von Oswald et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Transformers_Learn_In-Context_by_Gradient_Descent_von_Oswald_2023.pdf)

* [Transformers as Algorithms: Generalization and Stability in In-context Learning, Y. Li et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Transformers_as_Algorithms-Generalization_and_Stability_in_In-context_Learning_Li_2023.pdf)

* [What Can Transformers Learn In-Context? A Case Study of Simple Function Classes, S. Garg, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/What_Can_Transformers_Learn_In-Context_A_Case_Study_of_Simple_Function_Classes_Garg_2023.pdf)

    related repo: https://github.com/dtsip/in-context-learning

    related youtube presentation: https://www.youtube.com/watch?v=DiJsg93zQDc

* [The Transient Nature of Emergent In-Context Learning in Transformers, A. Singh et al, UCL, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/The_Transient_Nature_of_Emergent_In-Context_Learning_in_Transformers_Singh_UCL_2023.pdf)

* [Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions, S. Bhattamishra, Oxford U., 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Understanding_In-Context_Learning_in_Transformers_and_LLMs_by_Learning_to_Learn_Discrete_Functions_Bhattamishra_2023.pdf) 

* [Transformers Learn Higher-Order Optimization Methods for In-Context Learning: A Study with Linear Models, D. Fu et al, USC, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/in-context_learning/Transformers_Learn_Higher-Order_Optimization_Methods_for_In-Context_Learning-A_Study_with_Linear_Models_Fu_USC_2023.pdf)

* [Learning linear models in-context with transformers with Spencer Frei (UC Davis), Imperial College London, youtube video](https://www.youtube.com/watch?v=_lf7CD8zRyQ)

* [In-context Learning in Transformers - SLT Seminar 46, youtube video](https://www.youtube.com/watch?v=Y00HtwKO3BY)

## Reinforcement Learning in Transformers

* [Decision Transformer: Reinforcement Learning via Sequence Modeling, Lili Chen et al, UC Berkeley, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Decision_Transformer-Reinforcement_Learning_via_Sequence_Modeling_Chen_GoogleDeepMind_2021.pdf)

* [Stanford CS 25: Lecture 4 Decision Transformer: Reinforcement Learning via Sequence Modeling, youtube video](https://youtu.be/w4Bw8WYL8Ps)

* [Q-Transformer: Scalable Offline Reinforcement Learning via Autoregressive Q-Functions, Yevgen Chebotar et al, DeepMind, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Q-Transformer-Scalable_Offline_Reinforcement_Learning_via_Autoregressive_Q-Functions_Chebotar_DeepMind_2023.pdf)

    related repo: https://qtransformer.github.io/


## Hyper-Networks, MotherNet and PFNs (Prior-Data Fitted Networks)

* [HyperNetworks, David Ha, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/MotherNet/HyperNetworks_David_Ha_GoogleBrain_2017.pdf)

* [MotherNet: A Foundational Hypernetwork for Tabular Classification, A.C. Mueller et al, Microsoft Research, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/MotherNet/MotherNet-A_Foundational_Hypernetwork_for_Tabular_Classification_Mueller_2023.pdf)

* [Transformers Can Do Bayesian Inference, Samuel Mueller et al,U of Freiburg, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/MotherNet/Transformers_Can_Do_Bayesian_Inference_Mueller_2023.pdf)

* [TabPFN: A Transformer That Solves Small Tabular Classification Problems in a Second, Noah Hollman et al, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/MotherNet/TabPFN-A_Transformer_that_Solves_Small_Tabular_Classification_Problems_in_a_Second_Hollman_2023.pdf)


## Sequential Decision Modeling and Predictive Sequence Models

* [Stanford CS 25: Lecture 4 Decision Transformer: Reinforcement Learning via Sequence Modeling, youtube video](https://youtu.be/w4Bw8WYL8Ps)

* [Decision Transformer: Reinforcement Learning via Sequence Modeling, Lili Chen et al, UC Berkeley, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Decision_Transformer-Reinforcement_Learning_via_Sequence_Modeling_Chen_GoogleDeepMind_2021.pdf)

* [Decision Transformer: Reinforcement Learning via Sequence Modeling (Research Paper Explained), youtube video](https://youtu.be/-buULmf7dec)

* [Using Sequences of Life-events to Predict Human Lives, Germans Savcisens et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Using_Sequences_of_Life-events_to_Predict_Human_Lives_Savcisens_2023.pdf)


[... More articles on Transformers](https://github.com/dimitarpg13/transformers_intro/tree/main/articles_and_books)


## Vision Transformers
* [Visualizing Attention in Vision Transformer with Aryan Jadon](https://medium.com/@aryanjadon/visualizing-attention-in-vision-transformer-c871908d86de)

* [Vision Transformers, Explained, Skylar Jean Callis, Toward Data Science, Feb, 2024](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8)

* [Comparison of Convolutional Neural Networks and Vision Transformers (ViTs) with Illas Papastratis](https://medium.com/@iliaspapastratis/comparison-of-convolutional-neural-networks-and-vision-transformers-vits-a8fc5486c5be)

* [Do Vision Transformers See Like Convolutional Neural Networks? M. Raghu, Google Brain, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Do_Vision_Transformers_See_Like_Convolutional_Neural_Networks_Raghu_GoogleBrain_2021.pdf)

* [How Do Vision Transformers Work? N. Park et al, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/How_Do_Vision_Transformers_Work_Park_2022.pdf)

* [An Image is Worth 16X16 Wwords: Transformers for Image Recognition at Scale, A. Dosovitskiy, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [Transformers for Image Recognition at Scale, Nel Houlsby and Dirk Weissenborn, Dec 2020, blog](https://blog.research.google/2020/12/transformers-for-image-recognition-at.html)

* [Why Transformers are Slowly Replacing CNNs in Computer Vision? Pranoy Radhakrishnan, Aug 2021, Becoming Human: Artificial Intelligence Magazine](https://becominghuman.ai/transformers-in-vision-e2e87b739feb)

* [Vision Transformers (ViT) in Image Recognition – 2024 Guide, Gaudenz Boesch, viso.ai blog](https://viso.ai/deep-learning/vision-transformer-vit/)

## State Space Models (an alternative of Transformers)

* [Hungry Hungry Hippos: Towards Language Modeling with State Space Models, D. Fu, T. Dao, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/state_space_models/Hungry_Hungry_Hippos-Towards_Language_Modeling_with_State_Space_Models_Fu_Dao_Stanford_2023.pdf)

* [Mamba: Linear-Time Sequence Modeling with Selective State Spaces, A. Gu, T. Dao, CMU, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/state_space_models/Mamba-Linear-Time_Sequence_Modeling_with_Selective_State_Spaces_Gu_CMU_2023.pdf.pdf)

* [Mamba: Can it replace Transformers? Vishal Rajput, Medium Jan 8, 2024](https://medium.com/aiguys/mamba-can-it-replace-transformers-fe2032537916) \
  as a pdf file: [here](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/state_space_models/Mamba_Can_it_replace_Transformers_Vishal_Rajput_Jan_2024_Medium.pdf)  

* [Why Mamba was rejected? Joe El Khoury, Medium, Feb 28, 2024](https://medium.com/@jelkhoury880/why-mamba-was-rejected-9b2f05f2141c) \
  as a pdf file: [here](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/state_space_models/Why_Mamba_was_rejected_recently_at_the_ICLR_by_Joe_Elkhoury_Feb_2024_Medium.pdf)

## Medium
* [The A-Z of Transformers: Everything You Need to Know with François Porcher](https://towardsdatascience.com/the-a-z-of-transformers-everything-you-need-to-know-c9f214c619ac)
    
    related paper: [Neural Machine Translation by Jointly Learning to Align and Translate, D. Bahdanau et al, 2015](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/NeuralMachineTranslationByJointlyLearningToAlignAndTranslateBahdanau2015.pdf)

    related repo: [Transformers from Scratch](https://github.com/FrancoisPorcher/awesome-ai-tutorials/tree/main/NLP/007%20-%20Transformers%20From%20Scratch)

* [Transformers — Intuitively and Exhaustively Explained with Daniel Warfield](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)

* [De-coded: Transformers explained in plain English with Chris Hughes](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6429)

* [Explainable AI: Visualizing Attention in Transformers with Abby Morgan](https://generativeai.pub/explainable-ai-visualizing-attention-in-transformers-4eb931a2c0f8)

* [Transformers - The Bigger The Better? with Jordi Torres](https://towardsdatascience.com/transformers-the-bigger-the-better-19f39f222ee3)

* [How to Take Advantage of the New Disruptive AI Technology Called Transformers with Jordi Torres](https://torres-ai.medium.com/how-to-take-advantage-of-the-new-disruptive-ai-technology-called-transformers-9e57a26506cb)

* [Transformes: The New Gem of Deep Learning with Jordi Torres](https://torres-ai.medium.com/transformers-the-new-gem-of-deep-learning-d0ae04bc4a75)

* [Transfer Learning: The Democratization of Transformers with Jori Torres](https://torres-ai.medium.com/transfer-learning-the-democratization-of-transformers-1d2493b14883)

* [Visualizing Attention in Vision Transformer with Aryan Jadon](https://medium.com/@aryanjadon/visualizing-attention-in-vision-transformer-c871908d86de)

* [Vision Transformers, Explained, Skylar Jean Callis, Toward Data Science, Feb, 2024](https://towardsdatascience.com/vision-transformers-explained-a9d07147e4c8)

* [The Transformer Architecture of GPT Models with Beatriz Stollniz](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)

* [Learning Transformers Code First Part 1 - The Setup with Lily Hughs-Robinson](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

* [Learning Transformers Code First Part 2 - GPT Up Close and Personal with Lily Hughs-Robinson](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)

* [Were Abstract Painters The First Encoders with Wouter van Heeswijk](https://towardsdatascience.com/were-abstract-painters-the-first-encoders-49aa1e04ffd5)

* [Comparison of Convolutional Neural Networks and Vision Transformers (ViTs) with Illas Papastratis](https://medium.com/@iliaspapastratis/comparison-of-convolutional-neural-networks-and-vision-transformers-vits-a8fc5486c5be)

    related paper: [Do Vision Transformers See Like Convolutional Neural Networks? M. Raghu, Google Brain, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Do_Vision_Transformers_See_Like_Convolutional_Neural_Networks_Raghu_GoogleBrain_2021.pdf)

    related paper: [How Do Vision Transformers Work? N. Park et al, 2022](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/How_Do_Vision_Transformers_Work_Park_2022.pdf)

    related paper: [An Image is Worth 16X16 Wwords: Transformers for Image Recognition at Scale, A. Dosovitskiy, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

    related blog: https://blog.research.google/2020/12/transformers-for-image-recognition-at.html

    related blog: https://becominghuman.ai/transformers-in-vision-e2e87b739feb

    related blog: https://viso.ai/deep-learning/vision-transformer-vit/

* [Understanding Temporal Fusion Transformer with Mouna Labiadh](https://medium.com/@mouna.labiadh/understanding-temporal-fusion-transformer-9a7a4fcde74b)

    related article: [Temporal Fusion Transformer for Interpretable Multi-horizon Time Series Forecasting](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TemporalFusionTransformersforInterpretableMulti-horizonTimeSeriesForecasting_BrianLim_2021.pdf)

* [Forecasting book sales with Temporal Fusion Transformer with Mouna Labiadh](https://medium.com/@mouna.labiadh/forecasting-book-sales-with-temporal-fusion-transformer-dd482a7a257c)

* [Personalized Recommendations with Transformers with Enis Teper](https://medium.com/hepsiburada-data-science/personalized-recommendations-with-transformers-11c13cff2be)

* [Hidden Markov Models Simplified with Sanjay Dorairaj](https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab)

* [Rubik’s cubes and Markov chains with Eduardo Teste](https://towardsdatascience.com/a-random-walk-on-the-rubiks-cube-684dd304bf3e)

* [Implementing Seq2Seq Models for Efficient Time Series Forecasting with Max Brenner](https://medium.com/@maxbrenner-ai/implementing-seq2seq-models-for-efficient-time-series-forecasting-88dba1d66187)

## Classes and Lectures on Transformers

### Stanford CS 25

[Stanford CS 25 Home url](https://web.stanford.edu/class/cs25/)

[Stanford CS 25 Transformers United: 25 Lectures Set, youtube playlist](https://www.youtube.com/playlist?list=PLoROMvodv4rNiJRchCzutFw5ItR_Z27CM)

[Stanford CS 25: Lecture 1 Transformers United: DL Models that have revolutionized NLP, CV, RL](https://youtu.be/P127jhj-8-Y)

[Stanford CS 25: Lecture 2 Transformers in Language: The development of GPT Models, GPT3](https://youtu.be/qGkzHFllWDY)

[Stanford CS 25: Lecture 3 Transformers in Vision: Tackling problems in Computer Vision](https://youtu.be/BP5CM0YxbP8)

[Stanford CS 25: Lecture 4 Decision Transformer: Reinforcement Learning via Sequence Modeling](https://youtu.be/w4Bw8WYL8Ps)

[Stanford CS 25: Lecture 5 Mixture of Experts (MoE) paradigm and the Switch Transformer](https://youtu.be/U8J32Z3qV8s)

[Stanford CS 25: Lecture 6 DeepMind's Perceiver and Perceiver IO: new data family architecture](https://youtu.be/wTZ3o36lXoQ)

[Stanford CS 25: Lecture 7 Self Attention and Non-parametric transformers (NPTs)](https://youtu.be/zejXBg-2Vpk)

[Stanford CS 25: Lecture 8 Transformer Circuits, Induction Heads, In-Context Learning](https://youtu.be/pC4zRb_5noQ)

[Stanford CS 25: Lecture 9 Audio Research: Transformers for Applications in Audio, Speech, Music](https://youtu.be/wvE2n8u3drA)

[Stanford CS 25: Lecture 10 Represent part-whole hierarchies in a neural network, Geoff Hinton](https://youtu.be/CYaju6aCMoQ)

[Stanford CS 25: Lecture 11 Introduction to Transformers w/ Andrej Karpathy](https://youtu.be/XfpMkf4rD6E)

[Stanford CS 25: Lecture 12 Language and Human Alignment](https://youtu.be/DJ1Yy6Aquug)

[Stanford CS 25: Lecture 13 Emergent Abilities and Scaling in LLMs](https://youtu.be/tVtOevLrt5U)

[Stanford CS 25: Lecture 14 Strategic Games](https://youtu.be/phWxl0nkgKk)

[Stanford CS 25: Lecture 15 Robotics and Imitation Learning](https://youtu.be/ct4tdyyNDY4)

[Stanford CS 25: Lecture 16 Common Sense Reasoning](https://youtu.be/sTQaJyrI-zg)

[Stanford CS 25: Lecture 17 Biomedical Transformers](https://youtu.be/nz7_wg5iOlA)

[Stanford CS 25: Lecture 18 Neuroscience-Inspired Artificial Intelligence](https://youtu.be/L4DC7e6g2iI)

[Stanford CS 25: Lecture 19 Low-level Embodied Intelligence w/ Foundation Models](https://youtu.be/fz8wf9hN20c)

[Stanford CS 25: Lecture 20 Generalist Agents in Open-Ended Worlds](https://youtu.be/wwQ1LQA3RCU)

[Stanford CS 25: Lecture 21 How I Learned to Stop Worrying and Love the Transformer](https://youtu.be/1GbDTTK3aR4)

[Stanford CS 25: Lecture 22 Recipe for Training Helpful Chatbots](https://youtu.be/mcep6W8oB1I)

[Stanford CS 25: Lecture 23 No Language Left Behind: Scaling Human-Centered Machine Translation](https://youtu.be/ckNMsUuLryM)

[Stanford CS 25: Lecture 24 Beyond LLMs: Agents, Emergent Abilities, Intermediate-Guided Reasoning, BabyLM](https://youtu.be/ylEk1TE1uBo)

[Stanford CS 25: Lecture 25 Retrieval Augmented Language Models](https://youtu.be/mE7IDf2SmJg)

### Youtube videos and presentations

[Attention is all you need (Transformer) - Model explanation (including math), Inference and Training, Umar Jamil, 2023, youtube video](https://youtu.be/bCz4OMemCcA)

### GPT - DYI

GPT in 60 Lines of code:  https://jaykmody.com/blog/gpt-from-scratch/
