# Transformer Resources

## articles

* [Adam: A Method for Stochasitc Optimization, D. Kingma et al, 2014](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Adam-A_method_for_stochastic_optimization_Kingma_2014.pdf)

* [Choice of Plausible Alternatives: An Evaluation of Commonsense Causal Reasoning, Roemmele et al, 2011](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Choice_of_Plausible_Alternatives-An_Evaluation_of_Commonsense_Causal_Reasoning_Roemmele_2011.pdf)

* [Catastrophic Interference In Connectionist Networks: The Sequential Learning Problem, McCloskey, Cohen, 1989](https://github.com/dimitarpg13/large_language_models/blob/main/articles/Catastrophic_Interference_in_Connectionist_Networks-The_Sequential_Learning_Problem_Mccloskey_Cohen_1989.pdf)

* [Attention Is All You Need, Vaswani et al, Google Brain, 2017](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Attention-is-all-you-need-NIPS-2017.pdf)

* [The Annotated Transformer, 2018](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheAnnotatedTransformer.pdf)

* [The Illustrated Transformer, Jay Alamar's blog, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TheIllustratedTransformer%E2%80%93JayAlammar%E2%80%93Visualizing_machine_learning_one_concept_at_a_time.pdf)

* [Attention in Natural Language Processing, Galassi et al., 2020](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/AttentionInNaturalLanguageProcessing.pdf)

* [HyperAttention: Long-context Attention in Near-Linear Time, Insu Han et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/HyperAttention-Long-context_Attention_in_Near-Linear_Time_Han_2023.pdf)

* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, Devlin et al., Google AI, 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/BERTPretrainingofDeepBidirectionalTransformersforLanguageUnderstanding.pdf)

* [FAIRSEQ: A Fast, Extensible Toolkit for Sequence Modeling, Ott et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FairseqAFastExtensibleToolkitForSequenceModeling.pdf)

* [Autoencoders, Dor Bank et al, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Autoencoders.pdf)

* [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling, Chung et al., 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/EmpiricalEvaluationOfGatedRecurrentNeuralNetworksonSequenceModeling.pdf)

* [On the Properties of Neural Machine Translation: Encoder-Decoder Approaches, Cho et al., U de Montreal, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [Fundamentals of Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM) Network, A. Sherstinsky, 2021](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/FundamentalsOfRNNandLSTMNetwork.pdf)

* [A Decomposable Attention Model for Natural Language Inference, Parikh et al., Google Research, 2016](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/DecomposableAttentionModelforNaturalLanguageInferenceParikhUszkoreit2016.pdf)

* [Sequence to Sequence Learning with Neural Networks, Sutskever et al, Google Research, 2014](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/SequencetoSequenceLearningwithNeuralNetworksSutsekver2014.pdf)

* [Transforming Auto-encoders, G. Hinton, A. Krizhevsky, et al., 2011](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TransformingAutoencodersHinton.pdf)

* [Long Short-Term Memory, Sepp Hochreiter et al., 1997](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/LongShortTermMemory.pdf)

* [Understanding LSTM: a tutorial into Long Short-Term Memory, R. Staudemeyer et al., 2019](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TutorialOnLongShortTermMemory2019.pdf)

* [Meta-Transformer: A Unified Framework for Multimodal Learning, Zhang, Y, et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Meta-Transformer_Unified_Framework_for_Multi-Modal_Learning_2023.pdf)

* [Small-scale proxies for large-scale Transformer training instabilities, M. Wortsman et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/Small-scale_proxies_for_large-scale_Transformer_training_instabilities_Wortsman_2023.pdf)

* [Boolformer: Symbolic Regression of Logic Functions with Transformers, d'Ascoli et al, 2023](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/OnthePropertiesOfNeuralMachineTranslationEncoderDecoderApproaches.pdf)

* [... More articles on Transformers](https://github.com/dimitarpg13/transformers_intro/tree/main/articles_and_books)

## Medium
* [Transformers — Intuitively and Exhaustively Explained with Daniel Warfield](https://towardsdatascience.com/transformers-intuitively-and-exhaustively-explained-58a5c5df8dbb)

* [De-coded: Transformers explained in plain English with Chris Hughes](https://towardsdatascience.com/de-coded-transformers-explained-in-plain-english-877814ba6430)

* [Explainable AI: Visualizing Attention in Transformers with Abby Morgan](https://generativeai.pub/explainable-ai-visualizing-attention-in-transformers-4eb931a2c0f8)

* [Transformers - The Bigger The Better? with Jordi Torres](https://towardsdatascience.com/transformers-the-bigger-the-better-19f39f222ee3)

* [How to Take Advantage of the New Disruptive AI Technology Called Transformers with Jordi Torres](https://torres-ai.medium.com/how-to-take-advantage-of-the-new-disruptive-ai-technology-called-transformers-9e57a26506cb)

* [Transformes: The New Gem of Deep Learning with Jordi Torres](https://torres-ai.medium.com/transformers-the-new-gem-of-deep-learning-d0ae04bc4a75)

* [Transfer Learning: The Democratization of Transformers with Jori Torres](https://torres-ai.medium.com/transfer-learning-the-democratization-of-transformers-1d2493b14883)

* [Visualizing Attention in Vision Transformer with Aryan Jadon](https://medium.com/@aryanjadon/visualizing-attention-in-vision-transformer-c871908d86de)

* [The Transformer Architecture of GPT Models with Beatriz Stollniz](https://towardsdatascience.com/the-transformer-architecture-of-gpt-models-b8695b48728b)

* [Learning Transformers Code First Part 1 - The Setup with Lily Hughs-Robinson](https://towardsdatascience.com/nanogpt-learning-transformers-code-first-part-1-f2044cf5bca0)

* [Learning Transformers Code First Part 2 - GPT Up Close and Personal with Lily Hughs-Robinson](https://towardsdatascience.com/learning-transformers-code-first-part-2-gpt-up-close-and-personal-1635b52ae0d7)

* [Were Abstract Painters The First Encoders with Wouter van Heeswijk](https://towardsdatascience.com/were-abstract-painters-the-first-encoders-49aa1e04ffd5)

* [Understanding Temporal Fusion Transformer with Mouna Labiadh](https://medium.com/@mouna.labiadh/understanding-temporal-fusion-transformer-9a7a4fcde74b)

    related article: [Temporal Fusion Transformer for Interpretable Multi-horizon Time Series Forecasting](https://github.com/dimitarpg13/transformers_intro/blob/main/articles_and_books/TemporalFusionTransformersforInterpretableMulti-horizonTimeSeriesForecasting_BrianLim_2021.pdf)

* [Forecasting book sales with Temporal Fusion Transformer with Mouna Labiadh](https://medium.com/@mouna.labiadh/forecasting-book-sales-with-temporal-fusion-transformer-dd482a7a257c)

* [Personalized Recommendations with Transformers with Enis Teper](https://medium.com/hepsiburada-data-science/personalized-recommendations-with-transformers-11c13cff2be)

* [Hidden Markov Models Simplified with Sanjay Dorairaj](https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab)

* [Rubik’s cubes and Markov chains with Eduardo Teste](https://towardsdatascience.com/a-random-walk-on-the-rubiks-cube-684dd304bf3e)
